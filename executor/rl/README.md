# Reinforcement Learning

This folder contains different Reinforcement Learning algorithms which can be used for the execution of plans in dynamic and non-deterministic environments. The currently implemented algorithms are the following:
* **Q-Learning (QL)**. The Q-Learning algorithm can be used in discrete (small) environments with discrete actions. This algorithm is the simplest of the reinforcement learning algorithms and is applicable only in very limited situations.
* **Deep Q-Learning (DQL)**. The Deep Q-Learning algorithm can be used in large and continuous environments with discrete actions. This algorithm, described in the paper "(Playing Atari with Deep Reinforcement Learning)[https://arxiv.org/pdf/1312.5602.pdf]", uses a neural network to represent, through regression, the q-values for each action that can be executed in each state in input to the network itself. The advantages of this approach are many: a) the state is described through a set of *features*, allowing the algorithm to learn different information according to the values of the features; b) the features can be described through continuous values, allowing the management of dense environments; c) the use of the neural networks allows generalization, learning information about unvisited states which are similar to those visited during the exploration process.
* **Twin Delayed DDPG (TD3)**. The Twin Delayed Deep Deterministic Policy Gradient can be used in large and continuous environments with continuous actions. This algorithm, described in the paper "(Addressing Function Approximation Error in Actor-Critic Methods)[https://arxiv.org/pdf/1802.09477.pdf]", uses a neural network to represent, through regression, the qactions that can be executed in each state in input to the network itself. This algorithm has the same properties as the DQL algorithm but, unlike the latter, it uses a set of features to describe the actions which, therefore, can be represented in a dense space.